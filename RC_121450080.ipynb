{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NAMA : YUNAENA MARATUL KIROM\n",
        "\n",
        "#NIM : 121450080\n",
        "\n",
        "#KELAS : TBD RC"
      ],
      "metadata": {
        "id": "0-iLwl5QliPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A Dataset to Play With"
      ],
      "metadata": {
        "id": "3rZak0kqlx5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to the unzipped CIFAR data\n",
        "data_dir = Path(\"data/cifar-10-batches-py/\")\n",
        "\n",
        "# Unpickle function provided by the CIFAR hosts\n",
        "def unpickle(file):\n",
        "    with open(file, \"rb\") as fo:\n",
        "        dict = pickle.load(fo, encoding=\"bytes\")\n",
        "    return dict\n",
        "\n",
        "images, labels = [], []\n",
        "for batch in data_dir.glob(\"data_batch_*\"):\n",
        "    batch_data = unpickle(batch)\n",
        "    for i, flat_im in enumerate(batch_data[b\"data\"]):\n",
        "        im_channels = []\n",
        "        # Each image is flattened, with channels in order of R, G, B\n",
        "        for j in range(3):\n",
        "            im_channels.append(\n",
        "                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))\n",
        "            )\n",
        "        # Reconstruct the original image\n",
        "        images.append(np.dstack((im_channels)))\n",
        "        # Save the label\n",
        "        labels.append(batch_data[b\"labels\"][i])\n",
        "\n",
        "print(\"Loaded CIFAR-10 training set:\")\n",
        "print(f\" - np.shape(images)     {np.shape(images)}\")\n",
        "print(f\" - np.shape(labels)     {np.shape(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvtF05AGmNPf",
        "outputId": "09df4d48-e74a-45a4-e6d2-9229d8324e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded CIFAR-10 training set:\n",
            " - np.shape(images)     (0,)\n",
            " - np.shape(labels)     (0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Kode di atas adalah sebuah skrip Python yang bertujuan untuk memuat dataset CIFAR-10, yang merupakan kumpulan data gambar. serta merepresentasikan data tersebut dalam format yang sesuai untuk dilakukan proses pelatihan atau pengujian algoritma pembelajaran mesin. Pertama, diinisialisasi beberapa modul yang diperlukan, yaitu numpy untuk manipulasi data numerik, pickle untuk membaca dan menulis objek Python ke file, dan Path dari pathlib untuk memanipulasi path file dan direktori. Dilanjutkan dengan mendefinisikan path ke direktori yang berisi data CIFAR-10 yang telah di-unzip. Ini dilakukan menggunakan Path dari modul pathlib. Kemudian, ada sebuah fungsi bernama unpickle yang didefinisikan. Fungsi ini bertugas untuk membaca file yang sudah di-\"unpickle\" dan mengembalikan kamus (dictionary) yang berisi data.\n",
        "Setelah itu, dilakukan iterasi melalui setiap batch data dalam direktori data CIFAR menggunakan loop for. Setiap batch di-\"unpickled\" menggunakan fungsi unpickle yang telah didefinisikan sebelumnya.\n",
        "Selanjutnya, untuk setiap gambar dalam batch, iterasi dilakukan dengan mengambil gambar yang sudah di-\"flattened\" (diratakan) dalam bentuk array 1D. Kemudian, array ini dipecah menjadi tiga bagian sesuai dengan channel warna (R, G, B) masing-masing gambar.\n",
        "Dengan channel-channel tersebut, gambar yang asli direkonstruksi dengan menggunakan np.dstack untuk membuat stack array dengan tiga dimensi, merepresentasikan gambar berwarna.\n",
        "Selama iterasi, label untuk setiap gambar juga disimpan.\n",
        "Setelah semua batch diproses, cetaklah informasi tentang dataset CIFAR-10 yang telah dimuat, yaitu bentuk (shape) dari array gambar (images) dan array label (labels)."
      ],
      "metadata": {
        "id": "ii6gzLa_n-9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup for Storing Images on Disk"
      ],
      "metadata": {
        "id": "Gp0Xh4IcmWiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$ pip install Pillow"
      ],
      "metadata": {
        "id": "PlK6b2Gdmihm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "$ conda install -c conda-forge pillow"
      ],
      "metadata": {
        "id": "lg7aql5jmkVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7OOO4vtMov6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting Started With LMDB"
      ],
      "metadata": {
        "id": "5tW_tmGemo0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$ pip install lmdb"
      ],
      "metadata": {
        "id": "OOcEtUWcmrsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "$ conda install -c conda-forge python-lmdb"
      ],
      "metadata": {
        "id": "_Sqk_mfYmvKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Kode di atas menunjukkan dua perintah yang digunakan untuk menginstal dua paket Python yang diperlukan untuk menjalankan kode tertentu. Perintah pip install Pillow digunakan untuk menginstal paket Pillow. Pillow adalah sebuah library Python yang berguna untuk manipulasi gambar. Perintah conda install -c conda-forge python-lmdb digunakan untuk menginstal paket python-lmdb menggunakan Anaconda. Paket ini memungkinkan akses ke basis data berorientasi kunci yang cepat dengan menggunakan penyimpanan berbasis file."
      ],
      "metadata": {
        "id": "9UkeNgaZo5Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting Started With HDF5"
      ],
      "metadata": {
        "id": "25mqaTTLmx1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$ pip install h5py"
      ],
      "metadata": {
        "id": "iNzxbWlomvHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "$ conda install -c conda-forge h5py"
      ],
      "metadata": {
        "id": "8QaKFFIgmvEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing a Single Image"
      ],
      "metadata": {
        "id": "dXIm_L0nm6Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "disk_dir = Path(\"data/disk/\")\n",
        "lmdb_dir = Path(\"data/lmdb/\")\n",
        "hdf5_dir = Path(\"data/hdf5/\")"
      ],
      "metadata": {
        "id": "NuMAjBImmvBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disk_dir.mkdir(parents=True, exist_ok=True)\n",
        "lmdb_dir.mkdir(parents=True, exist_ok=True)\n",
        "hdf5_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "He8k4oebmu-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : kode tersebut memastikan bahwa ketiga direktori yang diperlukan untuk menyimpan data dalam format disk, LMDB, dan HDF5 tersedia, dan siap digunakan dalam proses selanjutnya. Pertama-tama, direktori data/disk/ dibuat menggunakan metode mkdir dari objek Path. Parameter parents=True digunakan untuk membuat direktori induk secara rekursif jika diperlukan, dan exist_ok=True digunakan untuk mengabaikan kesalahan jika direktori tersebut sudah ada.\n",
        "Langkah yang sama dilakukan untuk membuat direktori data/lmdb/ dan data/hdf5/ dengan menggunakan objek Path yang sesuai untuk masing-masing direktori."
      ],
      "metadata": {
        "id": "rpVCuq4xqn5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing to Disk"
      ],
      "metadata": {
        "id": "S7zWok_KnAyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import csv\n",
        "\n",
        "def store_single_disk(image, image_id, label):\n",
        "    \"\"\" Stores a single image as a .png file on disk.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        image_id    integer unique ID for image\n",
        "        label       image label\n",
        "    \"\"\"\n",
        "    Image.fromarray(image).save(disk_dir / f\"{image_id}.png\")\n",
        "\n",
        "    with open(disk_dir / f\"{image_id}.csv\", \"wt\") as csvfile:\n",
        "        writer = csv.writer(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        writer.writerow([label])"
      ],
      "metadata": {
        "id": "fnJvX0eymu7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing to LMDB"
      ],
      "metadata": {
        "id": "IJzieSaSnHf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR_Image:\n",
        "    def __init__(self, image, label):\n",
        "        # Dimensions of image for reconstruction - not really necessary\n",
        "        # for this dataset, but some datasets may include images of\n",
        "        # varying sizes\n",
        "        self.channels = image.shape[2]\n",
        "        self.size = image.shape[:2]\n",
        "\n",
        "        self.image = image.tobytes()\n",
        "        self.label = label\n",
        "\n",
        "    def get_image(self):\n",
        "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
        "        image = np.frombuffer(self.image, dtype=np.uint8)\n",
        "        return image.reshape(*self.size, self.channels)"
      ],
      "metadata": {
        "id": "IKycYp5Zmu4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lmdb\n",
        "import pickle\n",
        "\n",
        "def store_single_lmdb(image, image_id, label):\n",
        "    \"\"\" Stores a single image to a LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        image_id    integer unique ID for image\n",
        "        label       image label\n",
        "    \"\"\"\n",
        "    map_size = image.nbytes * 10\n",
        "\n",
        "    # Create a new LMDB environment\n",
        "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), map_size=map_size)\n",
        "\n",
        "    # Start a new write transaction\n",
        "    with env.begin(write=True) as txn:\n",
        "        # All key-value pairs need to be strings\n",
        "        value = CIFAR_Image(image, label)\n",
        "        key = f\"{image_id:08}\"\n",
        "        txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "selkxsvZmu1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Kelas CIFAR_Image dan fungsi store_single_lmdb bekerja sama untuk menyimpan gambar dan labelnya ke dalam basis data berorientasi kunci (key-value) LMDB."
      ],
      "metadata": {
        "id": "EeisF0VRrjyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing With HDF5"
      ],
      "metadata": {
        "id": "Xybd2rttnQjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def store_single_hdf5(image, image_id, label):\n",
        "    \"\"\" Stores a single image to an HDF5 file.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        image_id    integer unique ID for image\n",
        "        label       image label\n",
        "    \"\"\"\n",
        "    # Create a new HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{image_id}.h5\", \"w\")\n",
        "\n",
        "    # Create a dataset in the file\n",
        "    dataset = file.create_dataset(\n",
        "        \"image\", np.shape(image), h5py.h5t.STD_U8BE, data=image\n",
        "    )\n",
        "    meta_set = file.create_dataset(\n",
        "        \"meta\", np.shape(label), h5py.h5t.STD_U8BE, data=label\n",
        "    )\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "xoTa7CA0nOfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Fungsi store_single_hdf5 digunakan untuk menyimpan satu gambar ke dalam sebuah file HDF5, bersama dengan labelnya."
      ],
      "metadata": {
        "id": "bXyXAYxirw5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experiments for Storing a Single Image"
      ],
      "metadata": {
        "id": "S6T8i8fHnUa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_store_single_funcs = dict(\n",
        "    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5\n",
        ")"
      ],
      "metadata": {
        "id": "2PXwk-qonOV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Dictionary _store_single_funcs didefinisikan untuk menyimpan fungsi-fungsi yang bertujuan untuk menyimpan data gambar ke dalam berbagai format penyimpanan, yaitu disk, LMDB, dan HDF5. Setiap format penyimpanan memiliki sebuah kunci yang terkait dengan fungsi yang sesuai untuk menyimpan data ke dalam format tersebut."
      ],
      "metadata": {
        "id": "HkCF36U0sCpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "\n",
        "store_single_timings = dict()\n",
        "\n",
        "for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "    t = timeit(\n",
        "        \"_store_single_funcs[method](image, 0, label)\",\n",
        "        setup=\"image=images[0]; label=labels[0]\",\n",
        "        number=1,\n",
        "        globals=globals(),\n",
        "    )\n",
        "    store_single_timings[method] = t\n",
        "    print(f\"Method: {method}, Time usage: {t}\")"
      ],
      "metadata": {
        "id": "oJHMwkrSnON7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Kode di atas mengukur waktu yang diperlukan untuk menyimpan sebuah gambar dan labelnya menggunakan tiga metode penyimpanan yang berbeda: disk, LMDB, dan HDF5. Modul timeit diimpor dari timeit untuk mengukur waktu eksekusi fungsi.\n",
        "Sebuah dictionary bernama store_single_timings didefinisikan untuk menyimpan waktu eksekusi (dalam detik) untuk setiap metode penyimpanan.\n",
        "Dilakukan iterasi melalui tiga metode penyimpanan yang berbeda, yaitu \"disk\", \"lmdb\", dan \"hdf5\".\n",
        "Pada setiap iterasi, fungsi timeit() digunakan untuk mengukur waktu eksekusi dari fungsi penyimpanan yang sesuai untuk metode tersebut."
      ],
      "metadata": {
        "id": "2E2yiGLDsKay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing Many Images"
      ],
      "metadata": {
        "id": "2cAxm7qZndmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_many_disk(images, labels):\n",
        "    \"\"\" Stores an array of images to disk\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        images       images array, (N, 32, 32, 3) to be stored\n",
        "        labels       labels array, (N, 1) to be stored\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    # Save all the images one by one\n",
        "    for i, image in enumerate(images):\n",
        "        Image.fromarray(image).save(disk_dir / f\"{i}.png\")\n",
        "\n",
        "    # Save all the labels to the csv file\n",
        "    with open(disk_dir / f\"{num_images}.csv\", \"w\") as csvfile:\n",
        "        writer = csv.writer(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        for label in labels:\n",
        "            # This typically would be more than just one value per row\n",
        "            writer.writerow([label])\n",
        "\n",
        "def store_many_lmdb(images, labels):\n",
        "    \"\"\" Stores an array of images to LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        images       images array, (N, 32, 32, 3) to be stored\n",
        "        labels       labels array, (N, 1) to be stored\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    map_size = num_images * images[0].nbytes * 10\n",
        "\n",
        "    # Create a new LMDB DB for all the images\n",
        "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), map_size=map_size)\n",
        "\n",
        "    # Same as before â€” but let's write all the images in a single transaction\n",
        "    with env.begin(write=True) as txn:\n",
        "        for i in range(num_images):\n",
        "            # All key-value pairs need to be Strings\n",
        "            value = CIFAR_Image(images[i], labels[i])\n",
        "            key = f\"{i:08}\"\n",
        "            txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
        "    env.close()\n",
        "\n",
        "def store_many_hdf5(images, labels):\n",
        "    \"\"\" Stores an array of images to HDF5.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        images       images array, (N, 32, 32, 3) to be stored\n",
        "        labels       labels array, (N, 1) to be stored\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    # Create a new HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{num_images}_many.h5\", \"w\")\n",
        "\n",
        "    # Create a dataset in the file\n",
        "    dataset = file.create_dataset(\n",
        "        \"images\", np.shape(images), h5py.h5t.STD_U8BE, data=images\n",
        "    )\n",
        "    meta_set = file.create_dataset(\n",
        "        \"meta\", np.shape(labels), h5py.h5t.STD_U8BE, data=labels\n",
        "    )\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "M058770_nOCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Fungsi store_many_disk, store_many_lmdb, dan store_many_hdf5 adalah implementasi dari fungsi-fungsi yang bertujuan untuk menyimpan sejumlah gambar beserta labelnya ke dalam disk, LMDB, dan HDF5 secara berurutan."
      ],
      "metadata": {
        "id": "dx-h662TslRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the Dataset"
      ],
      "metadata": {
        "id": "fqJo5N7Jnkvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutoffs = [10, 100, 1000, 10000, 100000]\n",
        "\n",
        "# Let's double our images so that we have 100,000\n",
        "images = np.concatenate((images, images), axis=0)\n",
        "labels = np.concatenate((labels, labels), axis=0)\n",
        "\n",
        "# Make sure you actually have 100,000 images and labels\n",
        "print(np.shape(images))\n",
        "print(np.shape(labels))"
      ],
      "metadata": {
        "id": "THP2eEnknN42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : kode di atas, dilakukan penambahan jumlah gambar dan label menjadi 100.000 dengan menggandakan dataset yang ada. Kemudian, dilakukan pencetakan untuk memastikan bahwa jumlah gambar dan label benar-benar telah menjadi 100.000. List cutoffs didefinisikan dengan nilai-nilai [10, 100, 1000, 10000, 100000]. List ini mungkin digunakan sebagai nilai batas atau ambang yang akan digunakan dalam analisis atau pemrosesan data selanjutnya.\n",
        "Jumlah gambar dan label digandakan menjadi 100.000 dengan menggunakan fungsi np.concatenate() untuk menggabungkan dataset yang ada dengan dirinya sendiri pada sumbu pertama (axis=0). Hal ini dilakukan agar jumlah gambar dan label benar-benar menjadi 100.000.\n",
        "Dilakukan pencetakan untuk memastikan bahwa jumlah gambar dan label telah menjadi 100.000 dengan menggunakan np.shape() untuk mendapatkan bentuk dari array images dan labels, dan kemudian mencetaknya."
      ],
      "metadata": {
        "id": "ufIpuJQFsvsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experiment for Storing Many Images"
      ],
      "metadata": {
        "id": "sgObR8mAno65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_store_many_funcs = dict(\n",
        "    disk=store_many_disk, lmdb=store_many_lmdb, hdf5=store_many_hdf5\n",
        ")\n",
        "\n",
        "from timeit import timeit\n",
        "\n",
        "store_many_timings = {\"disk\": [], \"lmdb\": [], \"hdf5\": []}\n",
        "\n",
        "for cutoff in cutoffs:\n",
        "    for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "        t = timeit(\n",
        "            \"_store_many_funcs[method](images_, labels_)\",\n",
        "            setup=\"images_=images[:cutoff]; labels_=labels[:cutoff]\",\n",
        "            number=1,\n",
        "            globals=globals(),\n",
        "        )\n",
        "        store_many_timings[method].append(t)\n",
        "\n",
        "        # Print out the method, cutoff, and elapsed time\n",
        "        print(f\"Method: {method}, Time usage: {t}\")"
      ],
      "metadata": {
        "id": "AzBo6StgnNs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : kode di atas, dilakukan pengukuran waktu eksekusi untuk menyimpan sejumlah gambar dan label dalam tiga metode penyimpanan yang berbeda (disk, LMDB, dan HDF5) dengan menggunakan berbagai nilai ambang (cutoffs). Dictionary _store_many_funcs didefinisikan untuk menyimpan fungsi-fungsi yang bertujuan untuk menyimpan sejumlah gambar dan label dalam format penyimpanan yang berbeda: disk, LMDB, dan HDF5. Setiap format penyimpanan memiliki sebuah kunci yang terkait dengan fungsi yang sesuai untuk menyimpan data dalam format tersebut.\n",
        "Dictionary store_many_timings didefinisikan untuk menyimpan waktu eksekusi (dalam detik) untuk setiap metode penyimpanan, di mana kunci dictionary adalah nama metode penyimpanan (disk, lmdb, hdf5).\n",
        "Dilakukan perulangan melalui setiap nilai ambang dalam list cutoffs untuk menyimpan sejumlah gambar dan label yang berbeda.\n",
        "Pada setiap iterasi, dilakukan perulangan melalui tiga metode penyimpanan yang berbeda: disk, LMDB, dan HDF5.\n",
        "Fungsi timeit() digunakan untuk mengukur waktu eksekusi dari fungsi penyimpanan yang sesuai untuk metode tersebut."
      ],
      "metadata": {
        "id": "oIAncLoQtDSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_with_legend(\n",
        "    x_range, y_data, legend_labels, x_label, y_label, title, log=False\n",
        "):\n",
        "    \"\"\" Displays a single plot with multiple datasets and matching legends.\n",
        "        Parameters:\n",
        "        --------------\n",
        "        x_range         list of lists containing x data\n",
        "        y_data          list of lists containing y values\n",
        "        legend_labels   list of string legend labels\n",
        "        x_label         x axis label\n",
        "        y_label         y axis label\n",
        "    \"\"\"\n",
        "    plt.style.use(\"seaborn-whitegrid\")\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    if len(y_data) != len(legend_labels):\n",
        "        raise TypeError(\n",
        "            \"Error: number of data sets does not match number of labels.\"\n",
        "        )\n",
        "\n",
        "    all_plots = []\n",
        "    for data, label in zip(y_data, legend_labels):\n",
        "        if log:\n",
        "            temp, = plt.loglog(x_range, data, label=label)\n",
        "        else:\n",
        "            temp, = plt.plot(x_range, data, label=label)\n",
        "        all_plots.append(temp)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend(handles=all_plots)\n",
        "    plt.show()\n",
        "\n",
        "# Getting the store timings data to display\n",
        "disk_x = store_many_timings[\"disk\"]\n",
        "lmdb_x = store_many_timings[\"lmdb\"]\n",
        "hdf5_x = store_many_timings[\"hdf5\"]\n",
        "\n",
        "plot_with_legend(\n",
        "    cutoffs,\n",
        "    [disk_x, lmdb_x, hdf5_x],\n",
        "    [\"PNG files\", \"LMDB\", \"HDF5\"],\n",
        "    \"Number of images\",\n",
        "    \"Seconds to store\",\n",
        "    \"Storage time\",\n",
        "    log=False,\n",
        ")\n",
        "\n",
        "plot_with_legend(\n",
        "    cutoffs,\n",
        "    [disk_x, lmdb_x, hdf5_x],\n",
        "    [\"PNG files\", \"LMDB\", \"HDF5\"],\n",
        "    \"Number of images\",\n",
        "    \"Seconds to store\",\n",
        "    \"Log storage time\",\n",
        "    log=True,\n",
        ")"
      ],
      "metadata": {
        "id": "V9aZhjqanNkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Fungsi plot_with_legend digunakan untuk membuat plot yang menampilkan beberapa dataset dengan label-legenda yang sesuai. Fungsi menerima beberapa parameter:\n",
        "x_range: List dari list yang berisi data x.\n",
        "y_data: List dari list yang berisi data y.\n",
        "legend_labels: List dari string yang berisi label-legenda untuk setiap dataset.\n",
        "x_label: Label sumbu x.\n",
        "y_label: Label sumbu y.\n",
        "title: Judul plot.\n",
        "log: Argumen opsional untuk menentukan apakah plot akan menggunakan skala logaritmik atau tidak. Default-nya adalah False.\n",
        "Terlebih dahulu, gaya plot menggunakan plt.style.use(\"seaborn-whitegrid\") dan ukuran plot diatur menjadi figsize=(10, 7).\n",
        "Dilakukan pengecekan untuk memastikan bahwa jumlah dataset (y_data) sama dengan jumlah label-legenda (legend_labels). Jika tidak, akan muncul pesan kesalahan.\n",
        "Selanjutnya, dilakukan iterasi melalui y_data dan legend_labels. Untuk setiap pasangan data dan label-legenda, plot dibuat menggunakan plt.plot() atau plt.loglog() tergantung pada nilai parameter log. Hasil dari fungsi ini disimpan dalam variabel temp dan ditambahkan ke dalam list all_plots.\n",
        "Setelah selesai melakukan iterasi, judul plot, label sumbu x, dan label sumbu y ditambahkan dengan menggunakan plt.title(), plt.xlabel(), dan plt.ylabel().\n",
        "Legenda ditambahkan ke plot menggunakan plt.legend(), dengan menggunakan list all_plots sebagai inputnya.\n",
        "Plot ditampilkan dengan menggunakan plt.show()."
      ],
      "metadata": {
        "id": "SXv1pvVJtT6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading a Single Image"
      ],
      "metadata": {
        "id": "q_XiDOT1nxAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading From Disk"
      ],
      "metadata": {
        "id": "YwYFAeDZn0AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_single_disk(image_id):\n",
        "    \"\"\" Stores a single image to disk.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image_id    integer unique ID for image\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        label       associated meta data, int label\n",
        "    \"\"\"\n",
        "    image = np.array(Image.open(disk_dir / f\"{image_id}.png\"))\n",
        "\n",
        "    with open(disk_dir / f\"{image_id}.csv\", \"r\") as csvfile:\n",
        "        reader = csv.reader(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        label = int(next(reader)[0])\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "EpO93siKnNg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading From LMDB"
      ],
      "metadata": {
        "id": "am0jinfon6CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_single_lmdb(image_id):\n",
        "    \"\"\" Stores a single image to LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image_id    integer unique ID for image\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        label       associated meta data, int label\n",
        "    \"\"\"\n",
        "    # Open the LMDB environment\n",
        "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), readonly=True)\n",
        "\n",
        "    # Start a new read transaction\n",
        "    with env.begin() as txn:\n",
        "        # Encode the key the same way as we stored it\n",
        "        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
        "        # Remember it's a CIFAR_Image object that is loaded\n",
        "        cifar_image = pickle.loads(data)\n",
        "        # Retrieve the relevant bits\n",
        "        image = cifar_image.get_image()\n",
        "        label = cifar_image.label\n",
        "    env.close()\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "-81S6pw3nNcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Fungsi read_single_lmdb digunakan untuk membaca satu gambar dari basis data LMDB. ungsi menerima satu parameter:\n",
        "image_id: ID unik dalam bentuk integer untuk gambar yang akan dibaca.\n",
        "Lingkungan (environment) LMDB dibuka dalam mode hanya baca (readonly) menggunakan lmdb.open().\n",
        "Transaksi baca baru dimulai menggunakan env.begin(). Transaksi ini memungkinkan pembacaan data dari basis data.\n",
        "Kunci (key) yang sesuai dengan image_id dienkripsi menggunakan format yang sama seperti saat data disimpan dengan menggunakan f\"{image_id:08}\".encode(\"ascii\").\n",
        "Data yang sesuai dengan kunci tersebut dibaca dari basis data menggunakan txn.get(). Data yang dibaca adalah objek CIFAR_Image yang sebelumnya disimpan.\n",
        "Objek CIFAR_Image di-deserialize kembali menggunakan pickle.loads().\n",
        "Gambar dan label yang sesuai dengan objek CIFAR_Image dibaca dan diambil.\n",
        "Setelah selesai membaca data, lingkungan LMDB ditutup dengan menggunakan env.close().\n",
        "Gambar dan label yang dibaca kemudian dikembalikan sebagai output dari fungsi."
      ],
      "metadata": {
        "id": "SiPlnAddtsAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading From HDF5"
      ],
      "metadata": {
        "id": "M3fw7lxwn-rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_single_hdf5(image_id):\n",
        "    \"\"\" Stores a single image to HDF5.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image_id    integer unique ID for image\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        label       associated meta data, int label\n",
        "    \"\"\"\n",
        "    # Open the HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{image_id}.h5\", \"r+\")\n",
        "\n",
        "    image = np.array(file[\"/image\"]).astype(\"uint8\")\n",
        "    label = int(np.array(file[\"/meta\"]).astype(\"uint8\"))\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "a2yBgWiJmuvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Fungsi read_single_hdf5 bertujuan untuk membaca satu gambar dan label yang terkait dari file HDF5."
      ],
      "metadata": {
        "id": "LSsYA45pt6HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_read_single_funcs = dict(\n",
        "    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5\n",
        ")"
      ],
      "metadata": {
        "id": "cuBb9SP9musE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment for Reading a Single Image"
      ],
      "metadata": {
        "id": "e79_PyeGoHb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "\n",
        "read_single_timings = dict()\n",
        "\n",
        "for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "    t = timeit(\n",
        "        \"_read_single_funcs[method](0)\",\n",
        "        setup=\"image=images[0]; label=labels[0]\",\n",
        "        number=1,\n",
        "        globals=globals(),\n",
        "    )\n",
        "    read_single_timings[method] = t\n",
        "    print(f\"Method: {method}, Time usage: {t}\")"
      ],
      "metadata": {
        "id": "0DaLsOveoFO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : kode di atas mengukur waktu yang diperlukan untuk membaca satu gambar dan label terkait dari tiga metode penyimpanan yang berbeda (disk, LMDB, dan HDF5)."
      ],
      "metadata": {
        "id": "l9-pQ0C0uc-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading Many Images"
      ],
      "metadata": {
        "id": "hAUk2f6yoMEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusting the Code for Many Images"
      ],
      "metadata": {
        "id": "0TGSIgCpoO1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_many_disk(num_images):\n",
        "    \"\"\" Reads image from disk.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        num_images   number of images to read\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 32, 32, 3) to be stored\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "\n",
        "    # Loop over all IDs and read each image in one by one\n",
        "    for image_id in range(num_images):\n",
        "        images.append(np.array(Image.open(disk_dir / f\"{image_id}.png\")))\n",
        "\n",
        "    with open(disk_dir / f\"{num_images}.csv\", \"r\") as csvfile:\n",
        "        reader = csv.reader(\n",
        "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        for row in reader:\n",
        "            labels.append(int(row[0]))\n",
        "    return images, labels\n",
        "\n",
        "def read_many_lmdb(num_images):\n",
        "    \"\"\" Reads image from LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        num_images   number of images to read\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 32, 32, 3) to be stored\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), readonly=True)\n",
        "\n",
        "    # Start a new read transaction\n",
        "    with env.begin() as txn:\n",
        "        # Read all images in one single transaction, with one lock\n",
        "        # We could split this up into multiple transactions if needed\n",
        "        for image_id in range(num_images):\n",
        "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
        "            # Remember that it's a CIFAR_Image object\n",
        "            # that is stored as the value\n",
        "            cifar_image = pickle.loads(data)\n",
        "            # Retrieve the relevant bits\n",
        "            images.append(cifar_image.get_image())\n",
        "            labels.append(cifar_image.label)\n",
        "    env.close()\n",
        "    return images, labels\n",
        "\n",
        "def read_many_hdf5(num_images):\n",
        "    \"\"\" Reads image from HDF5.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        num_images   number of images to read\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 32, 32, 3) to be stored\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "\n",
        "    # Open the HDF5 file\n",
        "    file = h5py.File(hdf5_dir / f\"{num_images}_many.h5\", \"r+\")\n",
        "\n",
        "    images = np.array(file[\"/images\"]).astype(\"uint8\")\n",
        "    labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "_read_many_funcs = dict(\n",
        "    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5\n",
        ")"
      ],
      "metadata": {
        "id": "5nvm4K6JoFHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : Fungsi read_many_disk, read_many_lmdb, dan read_many_hdf5 bertujuan untuk membaca sejumlah gambar dan label yang terkait dari tiga metode penyimpanan yang berbeda: disk, LMDB, dan HDF5. Fungsi read_many_disk:\n",
        "\n",
        "Fungsi menerima satu parameter:\n",
        "num_images: Jumlah gambar yang akan dibaca.\n",
        "Array kosong images dan labels dideklarasikan untuk menampung gambar dan label yang akan dibaca.\n",
        "Dilakukan perulangan dari 0 hingga num_images - 1 untuk membaca setiap gambar secara berurutan dari disk.\n",
        "Setiap gambar dibuka menggunakan Image.open() dari modul PIL dan diubah menjadi array NumPy menggunakan np.array().\n",
        "Array gambar kemudian ditambahkan ke dalam list images.\n",
        "Label-label yang terkait dengan gambar-gambar tersebut dibaca dari file CSV terpisah dan disimpan ke dalam list labels.\n",
        "Setelah selesai membaca semua gambar dan label, keduanya dikembalikan sebagai output dari fungsi.\n",
        "\n",
        "Fungsi read_many_lmdb:\n",
        "\n",
        "Fungsi menerima satu parameter yang sama dengan fungsi read_many_disk.\n",
        "Array kosong images dan labels dideklarasikan untuk menampung gambar dan label yang akan dibaca.\n",
        "Lingkungan LMDB dibuka dalam mode hanya baca (readonly) menggunakan lmdb.open().\n",
        "Dilakukan perulangan dari 0 hingga num_images - 1 untuk membaca setiap gambar secara berurutan dari basis data LMDB.\n",
        "Setiap gambar dan label yang terkait dibaca menggunakan transaksi baca LMDB yang sesuai.\n",
        "Array gambar dan label kemudian ditambahkan ke dalam list images dan labels.\n",
        "Setelah selesai membaca semua gambar dan label, keduanya dikembalikan sebagai output dari fungsi.\n",
        "\n",
        "Fungsi read_many_hdf5:\n",
        "\n",
        "Fungsi menerima satu parameter yang sama dengan fungsi read_many_disk.\n",
        "Array kosong images dan labels dideklarasikan untuk menampung gambar dan label yang akan dibaca.\n",
        "File HDF5 dibuka menggunakan h5py.File() dalam mode baca-tulis (\"r+\").\n",
        "Data gambar dan label dibaca dari dataset yang sesuai di dalam file HDF5 menggunakan np.array().\n",
        "Data gambar dibaca dari dataset \"images\" dan disimpan dalam array images.\n",
        "Data label dibaca dari dataset \"meta\" dan disimpan dalam array labels.\n",
        "Setelah selesai membaca semua gambar dan label, keduanya dikembalikan sebagai output dari fungsi."
      ],
      "metadata": {
        "id": "EsXcZgaXvpeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment for Reading Many Images"
      ],
      "metadata": {
        "id": "HeLbF-_EoTYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "\n",
        "read_many_timings = {\"disk\": [], \"lmdb\": [], \"hdf5\": []}\n",
        "\n",
        "for cutoff in cutoffs:\n",
        "    for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
        "        t = timeit(\n",
        "            \"_read_many_funcs[method](num_images)\",\n",
        "            setup=\"num_images=cutoff\",\n",
        "            number=1,\n",
        "            globals=globals(),\n",
        "        )\n",
        "        read_many_timings[method].append(t)\n",
        "\n",
        "        # Print out the method, cutoff, and elapsed time\n",
        "        print(f\"Method: {method}, No. images: {cutoff}, Time usage: {t}\")"
      ],
      "metadata": {
        "id": "ULOruqMuoE-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : kode di atas digunakan untuk mengukur waktu yang diperlukan untuk membaca sejumlah gambar dan label yang berbeda dari tiga metode penyimpanan yang berbeda (disk, LMDB, dan HDF5) dengan menggunakan berbagai jumlah gambar (cutoffs)."
      ],
      "metadata": {
        "id": "BT7Pwk6nxZBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A More Critical Look at Implementation"
      ],
      "metadata": {
        "id": "JIu5Ro7OaQgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slightly slower\n",
        "for i in range(len(dataset)):\n",
        "    # Read the ith value in the dataset, one at a time\n",
        "    do_something_with(dataset[i])\n",
        "\n",
        "# This is better\n",
        "data = dataset[:]\n",
        "for d in data:\n",
        "    do_something_with(d)"
      ],
      "metadata": {
        "id": "pDyPRFJzoE1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis : kode di atas memberikan dua pendekatan yang berbeda dalam melakukan iterasi melalui dataset."
      ],
      "metadata": {
        "id": "sDYCE2BexhUD"
      }
    }
  ]
}